{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Snow Grooming Object Detection with PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN9Qg7B7rt8j9kz2Wp3UIQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenjkaplan/snow-grooming-object-detection/blob/master/Snow_Grooming_Object_Detection_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H_w-q1VQIWf",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcLa0OXeair",
        "colab_type": "text"
      },
      "source": [
        "##### Download additional utility files.\n",
        "\n",
        "Torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c1htRqPAWB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7fa7d1a2-cba1-4eed-8a6d-d6752d49eebd"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "HEAD is now at be37608 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTrv4qp-JrtG",
        "colab_type": "text"
      },
      "source": [
        "Get YouTube Downloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KHUChXdJnr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a5f88257-05c6-45aa-ec02-e0058469bec7"
      },
      "source": [
        "!git clone https://github.com/ankandrew/YT-Downloader-Trimmer.git ytdownloader    # for downloading and trimming videos\n",
        "!pip install -r /content/ytdownloader/requirements.txt            "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ytdownloader' already exists and is not an empty directory.\n",
            "Requirement already satisfied: youtube_dl==2020.6.16.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/ytdownloader/requirements.txt (line 1)) (2020.6.16.1)\n",
            "Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.6/dist-packages (from -r /content/ytdownloader/requirements.txt (line 2)) (0.2.3.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from moviepy==0.2.3.5->-r /content/ytdownloader/requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from moviepy==0.2.3.5->-r /content/ytdownloader/requirements.txt (line 2)) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy==0.2.3.5->-r /content/ytdownloader/requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy==0.2.3.5->-r /content/ytdownloader/requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0,>=2.1.2->moviepy==0.2.3.5->-r /content/ytdownloader/requirements.txt (line 2)) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehYEE6StDVlL",
        "colab_type": "text"
      },
      "source": [
        "##### Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3RSV7bE-RCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = '/content/drive/My Drive/Colab Notebooks/snow_grooming/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kql8NI9716Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import style\n",
        "import matplotlib.pyplot as plt\n",
        "style.use('fivethirtyeight')\n",
        "\n",
        "from PIL import Image\n",
        "from ytdownloader.downloader import Downloader\n",
        "\n",
        "import cv2\n",
        "import utils\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# custom modules\n",
        "import sys\n",
        "root_dir = '/content/drive/My Drive/Colab Notebooks/snow_grooming/'\n",
        "sys.path.append(root_dir)\n",
        "\n",
        "from utilities import get_object_detection_model, train_one_epoch, \\\n",
        "  get_validation_loss, evaluate\n",
        "from dataset import GoogleOpenImageDataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8JFQcxQDibi",
        "colab_type": "text"
      },
      "source": [
        "##### Make my Google Drive files available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROxROTdYWHg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1cf0f2ac-1efb-4ea2-d857-20e5cd133b7e"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t27HVa7ocK3",
        "colab_type": "text"
      },
      "source": [
        "##### Useful global variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI6AxLVajh6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj_class_labels = ['tree', 'person', 'street light']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9aFouMMmwOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = GoogleOpenImageDataset(root_dir, obj_class_labels, max_images_per_class=2000)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrNQrhBxfdYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "362c8f78-6e3b-48f6-9468-9f30bd179aeb"
      },
      "source": [
        "dataset[40]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0078, 0.0078, 0.0039,  ..., 0.1490, 0.1529, 0.1608],\n",
              "          [0.0157, 0.0118, 0.0118,  ..., 0.1451, 0.1569, 0.1647],\n",
              "          [0.0078, 0.0078, 0.0078,  ..., 0.1529, 0.1608, 0.1686],\n",
              "          ...,\n",
              "          [0.5882, 0.5686, 0.5765,  ..., 0.6431, 0.6431, 0.6392],\n",
              "          [0.5686, 0.5725, 0.5882,  ..., 0.6392, 0.6353, 0.6314],\n",
              "          [0.5765, 0.5804, 0.5961,  ..., 0.6235, 0.6196, 0.6157]],\n",
              " \n",
              "         [[0.3294, 0.3294, 0.3255,  ..., 0.4039, 0.4039, 0.4039],\n",
              "          [0.3373, 0.3333, 0.3333,  ..., 0.4000, 0.4078, 0.4078],\n",
              "          [0.3294, 0.3294, 0.3294,  ..., 0.4078, 0.4118, 0.4118],\n",
              "          ...,\n",
              "          [0.6118, 0.5922, 0.6000,  ..., 0.6549, 0.6549, 0.6510],\n",
              "          [0.5922, 0.5961, 0.6118,  ..., 0.6510, 0.6471, 0.6431],\n",
              "          [0.6000, 0.6039, 0.6196,  ..., 0.6353, 0.6314, 0.6275]],\n",
              " \n",
              "         [[0.4784, 0.4784, 0.4824,  ..., 0.5529, 0.5451, 0.5490],\n",
              "          [0.4863, 0.4824, 0.4902,  ..., 0.5490, 0.5490, 0.5529],\n",
              "          [0.4784, 0.4863, 0.4863,  ..., 0.5569, 0.5529, 0.5569],\n",
              "          ...,\n",
              "          [0.5569, 0.5373, 0.5451,  ..., 0.6196, 0.6196, 0.6235],\n",
              "          [0.5373, 0.5412, 0.5569,  ..., 0.6157, 0.6118, 0.6157],\n",
              "          [0.5451, 0.5490, 0.5647,  ..., 0.6000, 0.5961, 0.6000]]]),\n",
              " {'area': tensor([ 17340.,  26350.,  42984.,  43621.,  47929., 101260.]),\n",
              "  'boxes': tensor([[  0., 190.,  85., 394.],\n",
              "          [ 80., 183., 235., 353.],\n",
              "          [216., 162., 415., 378.],\n",
              "          [407., 176., 588., 417.],\n",
              "          [561., 167., 728., 454.],\n",
              "          [584., 220., 916., 525.]]),\n",
              "  'image_id': tensor([40]),\n",
              "  'iscrowd': tensor([0, 0, 0, 0, 0, 0]),\n",
              "  'labels': tensor([2, 2, 2, 2, 2, 2])})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhgsaHUOKcry",
        "colab_type": "text"
      },
      "source": [
        "### Modeling & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THS1vXqUjcQc",
        "colab_type": "text"
      },
      "source": [
        "##### Create training/validation/test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDllrnN_rZRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset_train = GoogleOpenImageDataset(root_dir, obj_class_labels, max_images_per_class=2000, train=True)\n",
        "dataset_val = GoogleOpenImageDataset(root_dir, obj_class_labels, max_images_per_class=2000)\n",
        "dataset_test = GoogleOpenImageDataset(root_dir, obj_class_labels, max_images_per_class=2000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9qE_4PlBJF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define train/val/test split (80/20 train_val/test and then 80/20 train/val)\n",
        "train_percent = 0.64\n",
        "val_percent = 0.16\n",
        "test_percent = 0.20\n",
        "total_size = len(dataset_train)\n",
        "train_size = int(train_percent*total_size)\n",
        "val_size = int(val_percent*total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "splits = [train_size, val_size, test_size]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HOYpkLCWiv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the dataset in train, val and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(total_size).tolist()\n",
        "train_idx, val_idx, test_idx = torch.utils.data.random_split(indices, splits)\n",
        "\n",
        "# make subsets based on train/val/test splits\n",
        "dataset_train = torch.utils.data.Subset(dataset_train, train_idx)\n",
        "dataset_val = torch.utils.data.Subset(dataset_val, val_idx)\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, test_idx)\n",
        "\n",
        "# TODO IMPORT UTILS USING STUFF IN OTHER NOTEBOOK, LOOKUP WHAT DATALOADER DOES\n",
        "# define training and validation data loaders\n",
        "data_loader_train = torch.utils.data.DataLoader(\n",
        "    dataset_train, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_val = torch.utils.data.DataLoader(\n",
        "    dataset_val, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N67F7lgzJsFs",
        "colab_type": "text"
      },
      "source": [
        "##### Define hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bcw_z5UJyw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer\n",
        "learning_rate = 0.005\n",
        "momentum = 0.9\n",
        "weight_decay = 0.0005\n",
        "\n",
        "# learning rate schedule\n",
        "step_size = 3   # learning rate will step every __ epochs\n",
        "gamma = 0.1    # learning rate will be multiplied by gamma every step \n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "trainable_layers = 3"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f09Au6TynJiA",
        "colab_type": "text"
      },
      "source": [
        "##### Train Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRDvEXG5XXug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcRxEpw-Atq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "bcd198b9-a57a-48f1-ad5a-e924f6428578"
      },
      "source": [
        "# Load Faster R-CNN Model pretrained on COCO and replace its classifier with a new one that has `num_classes`.\n",
        "model = get_object_detection_model(len(obj_class_labels) + 1, trainable_backbone_layers=trainable_layers) # extra class for background\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# create optimizer that will only train final layers\n",
        "optimizer = torch.optim.SGD(\n",
        "    params=[p for p in model.parameters() if p.requires_grad], \n",
        "    lr=learning_rate, \n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size,\n",
        "                                               gamma=gamma)\n",
        "\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    metric_logger, loss_tracker = train_one_epoch(model, optimizer, \n",
        "                                                  data_loader_train, device, \n",
        "                                                  epoch, print_freq=100)\n",
        "    \n",
        "    # save losses for plotting later\n",
        "    training_losses.append(np.mean(loss_tracker))\n",
        "    validation_loss = get_validation_loss(model, data_loader_val, device)\n",
        "    validation_losses.append(np.mean(validation_loss))\n",
        "    \n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_val, device=device)\n",
        "\n",
        "# plot loss curve\n",
        "plt.plot(training_losses)\n",
        "plt.plot(validation_losses)\n",
        "plt.title(f'Loss Curve (Learning Rate = {learning_rate})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/ops/boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  keep = keep.nonzero().squeeze(1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [   0/1920]  eta: 0:27:31  lr: 0.000010  loss: 1.5411 (1.5411)  loss_classifier: 1.1804 (1.1804)  loss_box_reg: 0.2661 (0.2661)  loss_objectness: 0.0758 (0.0758)  loss_rpn_box_reg: 0.0188 (0.0188)  time: 0.8603  data: 0.3723  max mem: 3104\n",
            "Epoch: [0]  [ 100/1920]  eta: 0:06:41  lr: 0.000509  loss: 0.4545 (0.8029)  loss_classifier: 0.1045 (0.3267)  loss_box_reg: 0.0608 (0.0831)  loss_objectness: 0.1504 (0.3311)  loss_rpn_box_reg: 0.0326 (0.0619)  time: 0.2137  data: 0.0073  max mem: 4037\n",
            "Epoch: [0]  [ 200/1920]  eta: 0:06:12  lr: 0.001009  loss: 0.2497 (0.5981)  loss_classifier: 0.0748 (0.2181)  loss_box_reg: 0.0469 (0.0784)  loss_objectness: 0.0919 (0.2480)  loss_rpn_box_reg: 0.0245 (0.0537)  time: 0.2106  data: 0.0068  max mem: 4037\n",
            "Epoch: [0]  [ 300/1920]  eta: 0:05:48  lr: 0.001508  loss: 0.2666 (0.5138)  loss_classifier: 0.0805 (0.1806)  loss_box_reg: 0.0557 (0.0761)  loss_objectness: 0.0658 (0.2054)  loss_rpn_box_reg: 0.0290 (0.0517)  time: 0.2160  data: 0.0068  max mem: 4037\n",
            "Epoch: [0]  [ 400/1920]  eta: 0:05:26  lr: 0.002008  loss: 0.2609 (0.4807)  loss_classifier: 0.0880 (0.1638)  loss_box_reg: 0.0660 (0.0772)  loss_objectness: 0.0686 (0.1891)  loss_rpn_box_reg: 0.0268 (0.0506)  time: 0.2146  data: 0.0073  max mem: 4412\n",
            "Epoch: [0]  [ 500/1920]  eta: 0:05:05  lr: 0.002507  loss: 0.2586 (0.4566)  loss_classifier: 0.0760 (0.1524)  loss_box_reg: 0.0316 (0.0770)  loss_objectness: 0.0703 (0.1765)  loss_rpn_box_reg: 0.0312 (0.0506)  time: 0.2172  data: 0.0071  max mem: 4413\n",
            "Epoch: [0]  [ 600/1920]  eta: 0:04:43  lr: 0.003007  loss: 0.2065 (0.4367)  loss_classifier: 0.0709 (0.1439)  loss_box_reg: 0.0487 (0.0756)  loss_objectness: 0.0606 (0.1686)  loss_rpn_box_reg: 0.0157 (0.0486)  time: 0.2119  data: 0.0069  max mem: 4413\n",
            "Epoch: [0]  [ 700/1920]  eta: 0:04:21  lr: 0.003506  loss: 0.2221 (0.4646)  loss_classifier: 0.0761 (0.1370)  loss_box_reg: 0.0554 (0.0737)  loss_objectness: 0.0582 (0.1699)  loss_rpn_box_reg: 0.0080 (0.0841)  time: 0.2237  data: 0.0071  max mem: 4413\n",
            "Epoch: [0]  [ 800/1920]  eta: 0:04:00  lr: 0.004006  loss: 0.2668 (0.4828)  loss_classifier: 0.0950 (0.1345)  loss_box_reg: 0.0409 (0.0737)  loss_objectness: 0.0741 (0.1664)  loss_rpn_box_reg: 0.0193 (0.1082)  time: 0.2260  data: 0.0074  max mem: 4413\n",
            "Epoch: [0]  [ 900/1920]  eta: 0:03:39  lr: 0.004505  loss: 0.2309 (0.4664)  loss_classifier: 0.0874 (0.1312)  loss_box_reg: 0.0577 (0.0736)  loss_objectness: 0.0719 (0.1610)  loss_rpn_box_reg: 0.0197 (0.1006)  time: 0.2089  data: 0.0069  max mem: 4413\n",
            "Epoch: [0]  [1000/1920]  eta: 0:03:18  lr: 0.005000  loss: 0.2447 (0.4552)  loss_classifier: 0.0784 (0.1278)  loss_box_reg: 0.0684 (0.0728)  loss_objectness: 0.0891 (0.1583)  loss_rpn_box_reg: 0.0164 (0.0962)  time: 0.2172  data: 0.0074  max mem: 4413\n",
            "Epoch: [0]  [1100/1920]  eta: 0:02:56  lr: 0.005000  loss: 0.2107 (0.4490)  loss_classifier: 0.0712 (0.1255)  loss_box_reg: 0.0542 (0.0727)  loss_objectness: 0.0641 (0.1538)  loss_rpn_box_reg: 0.0153 (0.0970)  time: 0.2091  data: 0.0072  max mem: 4413\n",
            "Epoch: [0]  [1200/1920]  eta: 0:02:35  lr: 0.005000  loss: 0.3051 (0.4444)  loss_classifier: 0.0906 (0.1246)  loss_box_reg: 0.0466 (0.0729)  loss_objectness: 0.0794 (0.1532)  loss_rpn_box_reg: 0.0226 (0.0937)  time: 0.2061  data: 0.0067  max mem: 4413\n",
            "Epoch: [0]  [1300/1920]  eta: 0:02:13  lr: 0.005000  loss: 0.2389 (0.4369)  loss_classifier: 0.0865 (0.1226)  loss_box_reg: 0.0458 (0.0727)  loss_objectness: 0.1003 (0.1517)  loss_rpn_box_reg: 0.0231 (0.0899)  time: 0.2141  data: 0.0073  max mem: 4413\n",
            "Epoch: [0]  [1400/1920]  eta: 0:01:52  lr: 0.005000  loss: 0.2152 (0.4292)  loss_classifier: 0.0794 (0.1206)  loss_box_reg: 0.0704 (0.0720)  loss_objectness: 0.0407 (0.1498)  loss_rpn_box_reg: 0.0142 (0.0868)  time: 0.2183  data: 0.0076  max mem: 4413\n",
            "Epoch: [0]  [1500/1920]  eta: 0:01:30  lr: 0.005000  loss: 0.1971 (0.4242)  loss_classifier: 0.0667 (0.1196)  loss_box_reg: 0.0251 (0.0720)  loss_objectness: 0.0643 (0.1488)  loss_rpn_box_reg: 0.0147 (0.0838)  time: 0.2104  data: 0.0076  max mem: 4413\n",
            "Epoch: [0]  [1600/1920]  eta: 0:01:08  lr: 0.005000  loss: 0.2192 (0.4633)  loss_classifier: 0.0865 (0.1187)  loss_box_reg: 0.0547 (0.0722)  loss_objectness: 0.0660 (0.1494)  loss_rpn_box_reg: 0.0174 (0.1229)  time: 0.2187  data: 0.0075  max mem: 4413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aewHHitK8cLH",
        "colab_type": "text"
      },
      "source": [
        "##### Evaluate on Test Set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7DyzlmM8bVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8E5vH904klE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " assert False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omg_Untbnceo",
        "colab_type": "text"
      },
      "source": [
        "#### Persist model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-sw2qvCgBdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = datetime.now()\n",
        "datetime_str = now.strftime(\"%m-%d-%Y %H%M\")\n",
        "torch.save(model, f'{root_dir}models/model_{datetime_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft7ylEUJnsBF",
        "colab_type": "text"
      },
      "source": [
        "### Prediction & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g8BbtDtFlNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 09-06-2020 0502 | training batch size = 2, learning rate = 0.001, min loss ~ 0.13, AP < 0.1\n",
        "# 09-08-2020 2041 | more data/slightly changed classes, learning rate = 0.01, \n",
        "# 09-09-2020 1658 | just tree, lr = 0.001\n",
        "# didn't save the next one, but it was lr = 0.01, did not improve perf\n",
        "# 09-09-2020 2305  | same as the last one I saved, pretty much. hadn't been evluating the model properly so redid it. \n",
        "# 09-10-2020 0437 | tree and person\n",
        "# 09-10-2020 1725 | tree, person, building, street light, 1000 images each\n",
        "# 09-11-2020 0039 | tree, person, street light, ~4000 images each , SGD optimizer, 60/20/20 split\n",
        "# 09-12-2020 1801 | tree, person, stree light, ~2000 images each, 64/16/20 split, added all bounding boxes\n",
        "\n",
        "model = torch.load('/content/drive/My Drive/Colab Notebooks/snow_grooming/models/model_09-11-2020 0039')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4uaUxZenmyV",
        "colab_type": "text"
      },
      "source": [
        "##### Pick an image from the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvzKpf5sE_Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_img_idx = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5PRka7wnpXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[test_img_idx]\n",
        "\n",
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOG_oLCnExWF",
        "colab_type": "text"
      },
      "source": [
        "##### Make a boundary box prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sth6vmXnxrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_boundary_box_prediction(image_no_box):\n",
        "# put the model in evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      prediction = model([image_no_box.to(device)])\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzq1s9DKqNbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_example = make_boundary_box_prediction(img)\n",
        "predict_example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boAukV5KE3DY",
        "colab_type": "text"
      },
      "source": [
        "##### Define function for drawing boundary box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3IkDZpd05T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_lookup_table = {\n",
        "    1: (obj_class_labels[0], (255, 0, 0)),\n",
        "    2: (obj_class_labels[1], (0, 255, 0)),\n",
        "    3: (obj_class_labels[2], (255, 255, 0)),\n",
        "}\n",
        "\n",
        "def draw_all_boundary_boxes(image_path, prediction, threshold=0.5):\n",
        "    # get boundary boxes, scores, and labels from prediction\n",
        "    boxes = prediction[0]['boxes'].tolist()\n",
        "    scores = prediction[0]['scores'].tolist()\n",
        "    class_labels = prediction[0]['labels'].tolist()    \n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    # im is a PIL Image object\n",
        "    #im_arr = np.asarray(image)\n",
        "    for box, score, label in zip(boxes, scores, class_labels):\n",
        "      # convert rgb array to opencv's bgr format\n",
        "      #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "      if score < threshold:\n",
        "        continue\n",
        "      x1 = int(box[0])\n",
        "      y1 = int(box[3])\n",
        "      x2 = int(box[2])\n",
        "      y2 = int(box[1])\n",
        "      # pts1 and pts2 are the upper left and bottom right coordinates of the rectangle\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), class_lookup_table[label][1], 3)\n",
        "      obj_label = 'pole' if class_lookup_table[label][0] == 'street light' else class_lookup_table[label][0]\n",
        "      cv2.putText(image, obj_label, (x1, y2-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, class_lookup_table[label][1], 2)\n",
        "    #im_arr = cv2.cvtColor(im_arr_bgr, cv2.COLOR_BGR2RGB)\n",
        "    #Image.fromarray(image)\n",
        "    return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-T9mGhB-yWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_img_idx_abs = test_idx[test_img_idx]\n",
        "path = dataset.imgs[test_img_idx_abs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SiiFG4Mz7y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = draw_all_boundary_boxes(path, predict_example)\n",
        "Image.fromarray(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubzv9i5CJFcH",
        "colab_type": "text"
      },
      "source": [
        "### Demo\n",
        "\n",
        "Demo on ski resort video footage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaD2CBc1MuNF",
        "colab_type": "text"
      },
      "source": [
        "##### Download video from YouTube.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wHxyTtQJLmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_url = \"https://www.youtube.com/watch?v=3tg_DOaUZ4Y\"\n",
        "#video_url = \"https://www.youtube.com/watch?v=LKBQ0J-RUF8\"\n",
        "video_quality = 1080 \n",
        "only_video = True \n",
        "do_trim = False \n",
        "start = \"00:28:16\" \n",
        "end = \"00:28:46\"\n",
        "output_file = 'pb600_winchcat_full.mp4'\n",
        "#output_file = 'groomer.mp4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFvQuiSGvk3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yt_d = Downloader(video_url, output_file, quality=video_quality, only_vid=only_video)\n",
        "yt_d.download()\n",
        "\n",
        "if do_trim:\n",
        "  yt_d.trim(start, end, delete_original=False)\n",
        "  os.rename('downloaded_vid_trimmed.mp4', 'downloaded_vid.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "low_a28Xtope",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for f in os.listdir():\n",
        "  if 'frame' in f:\n",
        "    os.remove(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mMPXxprMsdy",
        "colab_type": "text"
      },
      "source": [
        "##### Split video into frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G3AoKpFMy0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap = cv2.VideoCapture('pb600_winchcat_full.mp4')\n",
        "#cap = cv2.VideoCapture('groomer.mp4')\n",
        "i=0\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "        break\n",
        "    cv2.imwrite('frame'+str(i)+'.jpg',frame)\n",
        "    i+=1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(f'{i + 1} Frames Created.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Db4JLxPYeCS",
        "colab_type": "text"
      },
      "source": [
        "##### Draw boundary boxes on each frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUIb7aV1NNEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(i):\n",
        "  if idx % 100 == 0:\n",
        "    print(f'Making boundary box predictions ({idx}/{i})...')\n",
        "\n",
        "  # load image\n",
        "  img_frame = Image.open(f'frame{idx}.jpg').convert(\"RGB\")\n",
        "  transforms = get_transform(train=False)\n",
        "  # make prediction\n",
        "  prediction = make_boundary_box_prediction(transforms(img_frame))\n",
        "\n",
        "  # draw box\n",
        "  img_frame = draw_all_boundary_boxes(f'frame{idx}.jpg', prediction) \n",
        "\n",
        "  # resave image\n",
        "  cv2.imwrite(f'frame{idx}.jpg', img_frame)\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDq0ATJ019cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('frame6100.jpg').convert(\"RGB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJeFBFLIbM1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_frames_to_video(num_frames,path_out,fps):\n",
        "    frame_array = []\n",
        "    files = [f'frame{idx}.jpg' for idx in range(num_frames)]\n",
        " \n",
        "    #for sorting the file names properly\n",
        "    #files.sort(key = lambda x: int(x[5:-4]))\n",
        "    for f, filename in enumerate(files):\n",
        "        if f % 100 == 0:\n",
        "            print(f'Processing frame ({f}/{len(files)})...')\n",
        "        try:\n",
        "          #reading each files\n",
        "          img = cv2.imread(filename)\n",
        "          height, width, layers = img.shape\n",
        "          size = (width,height)\n",
        "          #inserting the frames into an image array\n",
        "          frame_array.append(img)\n",
        "        except AttributeError:\n",
        "          continue\n",
        " \n",
        "    out = cv2.VideoWriter(path_out,cv2.VideoWriter_fourcc(*'MJPG'), fps, size)\n",
        " \n",
        "    for i in range(len(frame_array)):\n",
        "        # writing to a image array\n",
        "        out.write(frame_array[i])\n",
        "    out.release()\n",
        "\n",
        "convert_frames_to_video(i + 1, root_dir + '/groomer_boxes.mp4', fps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARO60WKSdbEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}